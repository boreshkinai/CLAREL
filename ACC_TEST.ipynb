{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'PATH': '/opt/conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       "        'HOSTNAME': '78353253e378',\n",
       "        'TERM': 'xterm-color',\n",
       "        'NVIDIA_VISIBLE_DEVICES': '1',\n",
       "        'CUDA_VERSION': '9.0.176',\n",
       "        'CUDA_PKG_VERSION': '9-0=9.0.176-1',\n",
       "        'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\n",
       "        'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility',\n",
       "        'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0',\n",
       "        'NCCL_VERSION': '2.4.2',\n",
       "        'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\n",
       "        'CUDNN_VERSION': '7.4.2.24',\n",
       "        'LANG': 'C.UTF-8',\n",
       "        'HOME': '/root',\n",
       "        'JPY_PARENT_PID': '6',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
       "        'CUDA_VISIBLE_DEVICES': '1'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['NVIDIA_VISIBLE_DEVICES'] = \"1\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "os.environ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from train import ModelLoader, load_and_save_params, MetricLoader\n",
    "from datasets.cvpr2016_cub_loader import Cvpr2016CubLoader\n",
    "from common.util import Namespace\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import cKDTree, KDTree\n",
    "import time\n",
    "from common.metrics import ap_at_k_prototypes\n",
    "import os, pwd\n",
    "from common.pretrained_models import InceptionV3Loader, InceptionV2Loader\n",
    "from common.pretrained_models import INCEPTION_V2_PATH\n",
    "from PIL import Image\n",
    "from datasets.dataset_list import get_dataset_splits\n",
    "from common.metrics import get_prototypes, top1_gzsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /mnt/datasets/public/research/cvpr2016_cub/inception_v2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /mnt/datasets/public/research/cvpr2016_cub/inception_v2.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = InceptionV2Loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904487 9.62683\n",
      "251 251\n",
      "0.880126 9.28199\n",
      "388 388\n",
      "0.740908 8.68079\n",
      "0 0\n",
      "(1, 1024)\n"
     ]
    }
   ],
   "source": [
    "sample_images = ['dog.jpg', 'panda.jpg', 'tinca_tinca.jpg']\n",
    "\n",
    "for image in sample_images:\n",
    "    im = Image.open(image).resize((299,299))\n",
    "    im = np.array(im)\n",
    "    im = im.reshape(-1,299,299,3).astype(np.float32)\n",
    "    predict_values, logit_values, embedding = model.predict(im)\n",
    "    print (np.max(predict_values), np.max(logit_values))\n",
    "    print (np.argmax(predict_values)-1, np.argmax(logit_values)-1)\n",
    "    \n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /mnt/datasets/public/research/cvpr2016_cub/inception_v3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /mnt/datasets/public/research/cvpr2016_cub/inception_v3.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = InceptionV3Loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996392 13.0912\n",
      "251 251\n",
      "0.950092 10.0441\n",
      "388 388\n",
      "0.981881 11.0987\n",
      "0 0\n",
      "(1, 2048)\n"
     ]
    }
   ],
   "source": [
    "sample_images = ['dog.jpg', 'panda.jpg', 'tinca_tinca.jpg']\n",
    "\n",
    "for image in sample_images:\n",
    "    im = Image.open(image).resize((299,299))\n",
    "    im = np.array(im)\n",
    "    im = im.reshape(-1,299,299,3).astype(np.float32)\n",
    "    predict_values, logit_values, embedding = model.predict(im)\n",
    "    print (np.max(predict_values), np.max(logit_values))\n",
    "    print (np.argmax(predict_values)-1, np.argmax(logit_values)-1)\n",
    "    \n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test nearest neighbour model loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Searching for '/mnt/scratch/boris/experiments_zeroshot_pairwise/190508_220717_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_test_split_train_split_GZSL_results_test_split/embedding_size=512;test_split=test;train_split=trainval;dataset=xian2017_cub;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=9;modality_interaction=NONE/params.json'\n",
      "INFO:root:Loading existing params.\n",
      "INFO:root:Searching for '/mnt/scratch/boris/experiments_zeroshot_pairwise/190508_220717_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_test_split_train_split_GZSL_results_test_split/embedding_size=512;test_split=test;train_split=trainval;dataset=xian2017_cub;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=9;modality_interaction=NONE/params.json'\n",
      "INFO:root:Loading existing params.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Loading model\n",
      "INFO:tensorflow:Restoring parameters from /mnt/scratch/boris/experiments_zeroshot_pairwise/190508_220717_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_test_split_train_split_GZSL_results_test_split/embedding_size=512;test_split=test;train_split=trainval;dataset=xian2017_cub;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=9;modality_interaction=NONE/train/model-100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /mnt/scratch/boris/experiments_zeroshot_pairwise/190508_220717_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_test_split_train_split_GZSL_results_test_split/embedding_size=512;test_split=test;train_split=trainval;dataset=xian2017_cub;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=9;modality_interaction=NONE/train/model-100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_dir = 'batch_size-32-steps-100002-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101'\n",
    "log_dir = f\"logs/{model_dir}\"\n",
    "\n",
    "\n",
    "model_dir = 'mi_weight=0.4;consistency_loss=CLASSIFIER;embedding_size=512;num_texts=50;num_images=5;train_scheme=PAIRWISE;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=0;train_batch_size=32;modality_interaction=None'\n",
    "# model_dir = 'mi_weight=0.4;consistency_loss=CLASSIFIER;embedding_size=512;num_texts=10;num_images=5;train_scheme=FEWSHOT;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=0;train_batch_size=32;modality_interaction=None'\n",
    "log_dir = f\"/mnt/scratch/boris/experiments_zeroshot_pairwise/190419_115640_consistency_loss_embedding_size_num_images_train_scheme_text_feature_extractor_number_of_steps_repeat_train_batch_size_modality_interaction_mi_weight_num_texts_seen_unseen_test/{model_dir}\"\n",
    "\n",
    "# model_dir = 'mi_weight=0.4;embedding_size=512;dataset=xian2018_flowers;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=0;modality_interaction=NONE'\n",
    "# log_dir = f\"/mnt/scratch/boris/experiments_zeroshot_pairwise/190427_122014_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_mi_weight_xian2018_flowers_test/{model_dir}\"\n",
    "\n",
    "\n",
    "model_dir = 'mi_weight=0.5;embedding_size=512;test_split=test;train_split=trainval;dataset=xian2017_cub;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=9;modality_interaction=NONE'\n",
    "log_dir = f\"/mnt/scratch/boris/experiments_zeroshot_pairwise/190506_124706_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_mi_weight_test_split_train_split_GZSL_results_test_split/{model_dir}\"\n",
    "\n",
    "model_dir = 'embedding_size=512;test_split=test;train_split=trainval;dataset=xian2017_cub;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=9;modality_interaction=NONE'\n",
    "log_dir = f\"/mnt/scratch/boris/experiments_zeroshot_pairwise/190508_220717_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_test_split_train_split_GZSL_results_test_split/{model_dir}\"\n",
    "\n",
    "\n",
    "\n",
    "metric_model = MetricLoader(model_path=log_dir, batch_size_image=100, batch_size_text=200)\n",
    "\n",
    "feature_model = ModelLoader(model_path=log_dir, batch_size=None,\n",
    "                            num_images=metric_model.flags.num_images, num_texts=metric_model.flags.num_texts, \n",
    "                            max_text_len=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading split trainval\n",
      "INFO:root:Loading cached file /mnt/datasets/public/research/cvpr2016_cub/split_trainval_xian2017.pkl\n",
      "INFO:root:Loaded cache in 6.063078 sec\n",
      "INFO:root:Loading split test_seen\n",
      "INFO:root:Loading cached file /mnt/datasets/public/research/cvpr2016_cub/split_test_seen_xian2017.pkl\n",
      "INFO:root:Loaded cache in 2.655828 sec\n",
      "INFO:root:Loading split test_unseen\n",
      "INFO:root:Loading cached file /mnt/datasets/public/research/cvpr2016_cub/split_test_unseen_xian2017.pkl\n",
      "INFO:root:Loaded cache in 3.430594 sec\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = get_dataset_splits(dataset_name=metric_model.flags.dataset, data_dir=metric_model.flags.data_dir,\n",
    "                                    splits=[metric_model.flags.train_split, \n",
    "                                            metric_model.flags.test_split+\"_seen\", \n",
    "                                            metric_model.flags.test_split+\"_unseen\"], \n",
    "                                    flags=metric_model.flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing train embeddings\n",
      "221it [00:06, 35.94it/s]\n",
      "INFO:root:Computing test embeddings, unseen\n",
      "93it [00:02, 39.07it/s]\n",
      "INFO:root:Computing test embeddings, seen\n",
      "56it [00:01, 37.95it/s]\n",
      "INFO:root:Computing generalized zero-shot performance metrics\n",
      "INFO:root:Searching for '/mnt/scratch/boris/experiments_zeroshot_pairwise/190508_220717_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_test_split_train_split_GZSL_results_test_split/embedding_size=512;test_split=test;train_split=trainval;dataset=xian2017_cub;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=9;modality_interaction=NONE/params.json'\n",
      "INFO:root:Loading existing params.\n",
      " 17%|█▋        | 1/6 [00:00<00:00,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "****************\n",
      "UNSEEN images test\n",
      "****************\n",
      "=================\n",
      "STD seen prototype:\n",
      "62.7574590858\n",
      "STD unseen prototype:\n",
      "63.0824614458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:00<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype:\n",
      "61.7169943359\n",
      "STD unseen prototype:\n",
      "61.6395415988\n",
      "=================\n",
      "STD seen prototype:\n",
      "61.64822423\n",
      "STD unseen prototype:\n",
      "61.6026712354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:00<00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype:\n",
      "61.5992351442\n",
      "STD unseen prototype:\n",
      "61.4335828686\n",
      "=================\n",
      "STD seen prototype:\n",
      "61.5355217391\n",
      "STD unseen prototype:\n",
      "61.4173515602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  7.13it/s]\n",
      " 17%|█▋        | 1/6 [00:00<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype:\n",
      "61.5230058585\n",
      "STD unseen prototype:\n",
      "61.3917476117\n",
      "****************\n",
      "SEEN images test\n",
      "****************\n",
      "=================\n",
      "STD seen prototype:\n",
      "62.0299223077\n",
      "STD unseen prototype:\n",
      "64.7997361227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:00<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype:\n",
      "60.6836526257\n",
      "STD unseen prototype:\n",
      "63.6787015737\n",
      "=================\n",
      "STD seen prototype:\n",
      "60.5414230017\n",
      "STD unseen prototype:\n",
      "63.4785442258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:00<00:00,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype:\n",
      "60.4250671122\n",
      "STD unseen prototype:\n",
      "63.4299141033\n",
      "=================\n",
      "STD seen prototype:\n",
      "60.3792289105\n",
      "STD unseen prototype:\n",
      "63.3792244242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  9.29it/s]\n",
      "INFO:root:Computing classical zero-shot performance metrics, test\n",
      "INFO:root:Searching for '/mnt/scratch/boris/experiments_zeroshot_pairwise/190508_220717_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_test_split_train_split_GZSL_results_test_split/embedding_size=512;test_split=test;train_split=trainval;dataset=xian2017_cub;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=9;modality_interaction=NONE/params.json'\n",
      "INFO:root:Loading existing params.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype:\n",
      "60.3658705275\n",
      "STD unseen prototype:\n",
      "63.3989551743\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing classical zero-shot performance metrics, train\n",
      "INFO:root:Searching for '/mnt/scratch/boris/experiments_zeroshot_pairwise/190508_220717_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_test_split_train_split_GZSL_results_test_split/embedding_size=512;test_split=test;train_split=trainval;dataset=xian2017_cub;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=9;modality_interaction=NONE/params.json'\n",
      "INFO:root:Loading existing params.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_AP@50/#sentences10': 0.32813333333333333,\n",
       " 'train_Top-1 Acc/#sentences10': 0.46605845184163047,\n",
       " 'train_AP@50/#sentences50': 0.44586666666666663,\n",
       " 'train_Top-1 Acc/#sentences50': 0.70231396771063315,\n",
       " 'train_AP@50/#sentences100': 0.46293333333333331,\n",
       " 'train_Top-1 Acc/#sentences100': 0.74451485044499388,\n",
       " 'train_AP@50/#sentences200': 0.46879999999999994,\n",
       " 'train_Top-1 Acc/#sentences200': 0.77205143990811653,\n",
       " 'train_AP@50/#sentences400': 0.47426666666666678,\n",
       " 'train_Top-1 Acc/#sentences400': 0.78884844868922821,\n",
       " 'train_AP@50/#sentences1000': 0.47373333333333334,\n",
       " 'train_Top-1 Acc/#sentences1000': 0.7934936279164273,\n",
       " 'test_AP@50/#sentences10': 0.33839999999999998,\n",
       " 'test_Top-1 Acc/#sentences10': 0.42058009334452878,\n",
       " 'test_AP@50/#sentences50': 0.41639999999999994,\n",
       " 'test_Top-1 Acc/#sentences50': 0.58912191280667392,\n",
       " 'test_AP@50/#sentences100': 0.4204,\n",
       " 'test_Top-1 Acc/#sentences100': 0.63651968174850848,\n",
       " 'test_AP@50/#sentences200': 0.43480000000000002,\n",
       " 'test_Top-1 Acc/#sentences200': 0.64882804678568928,\n",
       " 'test_AP@50/#sentences400': 0.43639999999999995,\n",
       " 'test_Top-1 Acc/#sentences400': 0.6601181529272826,\n",
       " 'test_AP@50/#sentences1000': 0.44159999999999999,\n",
       " 'test_Top-1 Acc/#sentences1000': 0.67071177214398869,\n",
       " 'test_U_Top-1 Acc/#sentences10': 0.31029087112983322,\n",
       " 'test_S_Top-1 Acc/#sentences10': 0.32906815025932673,\n",
       " 'test_H_Top-1 Acc/#sentences10': 0.31940377656108732,\n",
       " 'test_U_Top-1 Acc/#sentences50': 0.50371144457694861,\n",
       " 'test_S_Top-1 Acc/#sentences50': 0.45441227345639107,\n",
       " 'test_H_Top-1 Acc/#sentences50': 0.47779354250000811,\n",
       " 'test_U_Top-1 Acc/#sentences100': 0.52910220627271454,\n",
       " 'test_S_Top-1 Acc/#sentences100': 0.48367313133489614,\n",
       " 'test_H_Top-1 Acc/#sentences100': 0.50536878496398896,\n",
       " 'test_U_Top-1 Acc/#sentences200': 0.56715094791221954,\n",
       " 'test_S_Top-1 Acc/#sentences200': 0.51009252022487317,\n",
       " 'test_H_Top-1 Acc/#sentences200': 0.53711062526795994,\n",
       " 'test_U_Top-1 Acc/#sentences400': 0.58037553949052245,\n",
       " 'test_S_Top-1 Acc/#sentences400': 0.52218332919803501,\n",
       " 'test_H_Top-1 Acc/#sentences400': 0.54974376426130489,\n",
       " 'test_U_Top-1 Acc/#sentences1000': 0.58955207696302614,\n",
       " 'test_S_Top-1 Acc/#sentences1000': 0.53578636124224355,\n",
       " 'test_H_Top-1 Acc/#sentences1000': 0.56138482674171175}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_eval, embeddings = feature_model.eval_acc_gzsh(train_loader=dataset_splits[metric_model.flags.train_split], \n",
    "                                                       test_loader_seen=dataset_splits[metric_model.flags.test_split+\"_seen\"],\n",
    "                                                       test_loader_unseen=dataset_splits[metric_model.flags.test_split+\"_unseen\"],\n",
    "                                                       batch_size=32, seen_adjustment=0.02) # 0.015\n",
    "results_eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute correction factor on the validation subset, CUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Searching for 'logs/batch_size-32-steps-100006-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/params.json'\n",
      "INFO:root:Loading existing params.\n",
      "INFO:root:Searching for 'logs/batch_size-32-steps-100006-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/params.json'\n",
      "INFO:root:Loading existing params.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Loading model\n",
      "INFO:tensorflow:Restoring parameters from logs/batch_size-32-steps-100006-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/train/model-100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/batch_size-32-steps-100006-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/train/model-100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_dir = 'batch_size-32-steps-100006-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101'\n",
    "log_dir = f\"logs/{model_dir}\"\n",
    "\n",
    "# model_dir = 'mi_weight=0.4;embedding_size=512;dataset=xian2018_flowers;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=0;modality_interaction=FILM_T'\n",
    "# log_dir = f\"/mnt/scratch/boris/experiments_zeroshot_pairwise/190427_122014_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_mi_weight_xian2018_flowers_test/{model_dir}\"\n",
    "\n",
    "\n",
    "metric_model = MetricLoader(model_path=log_dir, batch_size_image=100, batch_size_text=200)\n",
    "\n",
    "feature_model = ModelLoader(model_path=log_dir, batch_size=None,\n",
    "                            num_images=metric_model.flags.num_images, num_texts=metric_model.flags.num_texts, \n",
    "                            max_text_len=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading split train\n",
      "INFO:root:Loading cached file /mnt/datasets/public/research/cvpr2016_cub/split_train_xian2017.pkl\n",
      "INFO:root:Loaded cache in 4.652322 sec\n",
      "INFO:root:Loading split val_seen\n",
      "INFO:root:Loading cached file /mnt/datasets/public/research/cvpr2016_cub/split_val_seen_xian2017.pkl\n",
      "INFO:root:Loaded cache in 2.488875 sec\n",
      "INFO:root:Loading split val_unseen\n",
      "INFO:root:Loading cached file /mnt/datasets/public/research/cvpr2016_cub/split_val_unseen_xian2017.pkl\n",
      "INFO:root:Loaded cache in 3.572294 sec\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = get_dataset_splits(dataset_name=metric_model.flags.dataset, data_dir=metric_model.flags.data_dir,\n",
    "                                    splits=[metric_model.flags.train_split, \n",
    "                                            metric_model.flags.test_split+\"_seen\", \n",
    "                                            metric_model.flags.test_split+\"_unseen\"], \n",
    "                                    flags=metric_model.flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing train embeddings\n",
      "147it [00:03, 39.37it/s]\n",
      "INFO:root:Computing test embeddings, unseen\n",
      "93it [00:02, 42.48it/s]\n",
      "INFO:root:Computing test embeddings, seen\n",
      "37it [00:00, 40.24it/s]\n",
      "INFO:root:Computing generalized zero-shot performance metrics\n",
      "INFO:root:Searching for 'logs/batch_size-32-steps-100006-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/params.json'\n",
      "INFO:root:Loading existing params.\n",
      " 17%|█▋        | 1/6 [00:00<00:00,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "****************\n",
      "UNSEEN images test\n",
      "****************\n",
      "=================\n",
      "STD seen prototype:\n",
      "45.8416163628\n",
      "STD unseen prototype:\n",
      "45.7272962191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:00<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype:\n",
      "45.2667278979\n",
      "STD unseen prototype:\n",
      "45.3625948194\n",
      "=================\n",
      "STD seen prototype:\n",
      "45.1561514952\n",
      "STD unseen prototype:\n",
      "45.218320328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:00<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype:\n",
      "45.1231808041\n",
      "STD unseen prototype:\n",
      "45.1522787399\n",
      "=================\n",
      "STD seen prototype:\n",
      "45.1191111801\n",
      "STD unseen prototype:\n",
      "45.1502165409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  9.38it/s]\n",
      " 33%|███▎      | 2/6 [00:00<00:00, 14.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype:\n",
      "45.1304021464\n",
      "STD unseen prototype:\n",
      "45.1453882308\n",
      "****************\n",
      "SEEN images test\n",
      "****************\n",
      "=================\n",
      "STD seen prototype:\n",
      "45.4039450411\n",
      "STD unseen prototype:\n",
      "47.0649412148\n",
      "=================\n",
      "STD seen prototype:\n",
      "44.5498205594\n",
      "STD unseen prototype:\n",
      "46.6015820034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 15.31it/s]\n",
      "INFO:root:Computing classical zero-shot performance metrics, test\n",
      "INFO:root:Searching for 'logs/batch_size-32-steps-100006-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/params.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype:\n",
      "44.4055684942\n",
      "STD unseen prototype:\n",
      "46.6324181583\n",
      "=================\n",
      "STD seen prototype:\n",
      "44.360481148\n",
      "STD unseen prototype:\n",
      "46.5700480087\n",
      "=================\n",
      "STD seen prototype:\n",
      "44.3390915255\n",
      "STD unseen prototype:\n",
      "46.5877664401\n",
      "=================\n",
      "STD seen prototype:\n",
      "44.3413908277\n",
      "STD unseen prototype:\n",
      "46.5935012253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading existing params.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing classical zero-shot performance metrics, train\n",
      "INFO:root:Searching for 'logs/batch_size-32-steps-100006-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/params.json'\n",
      "INFO:root:Loading existing params.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_AP@50/#sentences10': 0.38319999999999993,\n",
       " 'train_Top-1 Acc/#sentences10': 0.53894202808253033,\n",
       " 'train_AP@50/#sentences50': 0.46140000000000003,\n",
       " 'train_Top-1 Acc/#sentences50': 0.78636570956544216,\n",
       " 'train_AP@50/#sentences100': 0.49800000000000005,\n",
       " 'train_Top-1 Acc/#sentences100': 0.83721254914492649,\n",
       " 'train_AP@50/#sentences200': 0.49800000000000005,\n",
       " 'train_Top-1 Acc/#sentences200': 0.86598500401185918,\n",
       " 'train_AP@50/#sentences400': 0.502,\n",
       " 'train_Top-1 Acc/#sentences400': 0.88435971215905607,\n",
       " 'train_AP@50/#sentences1000': 0.50140000000000007,\n",
       " 'train_Top-1 Acc/#sentences1000': 0.88538158213947671,\n",
       " 'test_AP@50/#sentences10': 0.27399999999999997,\n",
       " 'test_Top-1 Acc/#sentences10': 0.4156715532706311,\n",
       " 'test_AP@50/#sentences50': 0.28000000000000003,\n",
       " 'test_Top-1 Acc/#sentences50': 0.56310700548211112,\n",
       " 'test_AP@50/#sentences100': 0.29239999999999999,\n",
       " 'test_Top-1 Acc/#sentences100': 0.61088339272974501,\n",
       " 'test_AP@50/#sentences200': 0.28999999999999998,\n",
       " 'test_Top-1 Acc/#sentences200': 0.64087099274064541,\n",
       " 'test_AP@50/#sentences400': 0.30119999999999997,\n",
       " 'test_Top-1 Acc/#sentences400': 0.6659582179442366,\n",
       " 'test_AP@50/#sentences1000': 0.29799999999999999,\n",
       " 'test_Top-1 Acc/#sentences1000': 0.67029377434883164,\n",
       " 'test_U_Top-1 Acc/#sentences10': 0.30328861505361215,\n",
       " 'test_S_Top-1 Acc/#sentences10': 0.35021640777523139,\n",
       " 'test_H_Top-1 Acc/#sentences10': 0.32506758348518383,\n",
       " 'test_U_Top-1 Acc/#sentences50': 0.43419633053547962,\n",
       " 'test_S_Top-1 Acc/#sentences50': 0.52412347293229655,\n",
       " 'test_H_Top-1 Acc/#sentences50': 0.47494059471842509,\n",
       " 'test_U_Top-1 Acc/#sentences100': 0.50869681844498904,\n",
       " 'test_S_Top-1 Acc/#sentences100': 0.56063977933830877,\n",
       " 'test_H_Top-1 Acc/#sentences100': 0.5334067357917055,\n",
       " 'test_U_Top-1 Acc/#sentences200': 0.52804834088390895,\n",
       " 'test_S_Top-1 Acc/#sentences200': 0.57615136823960356,\n",
       " 'test_H_Top-1 Acc/#sentences200': 0.55105208158116947,\n",
       " 'test_U_Top-1 Acc/#sentences400': 0.54452091017026005,\n",
       " 'test_S_Top-1 Acc/#sentences400': 0.59545895607660315,\n",
       " 'test_H_Top-1 Acc/#sentences400': 0.5688518934976533,\n",
       " 'test_U_Top-1 Acc/#sentences1000': 0.55926458822699887,\n",
       " 'test_S_Top-1 Acc/#sentences1000': 0.59874020260784966,\n",
       " 'test_H_Top-1 Acc/#sentences1000': 0.57832954667660763}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_eval, embeddings = feature_model.eval_acc_gzsh(train_loader=dataset_splits[metric_model.flags.train_split], \n",
    "                                                       test_loader_seen=dataset_splits[metric_model.flags.test_split+\"_seen\"],\n",
    "                                                       test_loader_unseen=dataset_splits[metric_model.flags.test_split+\"_unseen\"],\n",
    "                                                       batch_size=32, seen_adjustment=0.02) # 0.015\n",
    "results_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0743413929989174"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "89.31 / 83.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.37279391,  0.02427058, -2.69599867, ...,  0.0203332 ,\n",
       "        -1.44127524,  0.30021363],\n",
       "       [ 1.02579761, -1.42297983, -0.12612197, ..., -0.93158758,\n",
       "         0.07975166, -2.19573784],\n",
       "       [ 1.19220066,  1.97308278, -2.59228873, ..., -0.53009856,\n",
       "         0.70677149, -1.83169401],\n",
       "       ..., \n",
       "       [-0.99557626,  0.10689936,  0.21294507, ...,  0.28846174,\n",
       "        -0.97834027, -1.66222882],\n",
       "       [-0.57412851,  2.34132791, -1.39146113, ...,  1.85419559,\n",
       "        -2.26026034, -2.53814721],\n",
       "       [-0.65821981,  2.30379725,  0.2592791 , ...,  1.03106701,\n",
       "         0.76612294, -1.74685097]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute correction factor on the validation subset, FLOWERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Searching for 'logs/batch_size-32-steps-100005-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/params.json'\n",
      "INFO:root:Loading existing params.\n",
      "INFO:root:Searching for 'logs/batch_size-32-steps-100005-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/params.json'\n",
      "INFO:root:Loading existing params.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Loading model\n",
      "INFO:tensorflow:Restoring parameters from logs/batch_size-32-steps-100005-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/train/model-100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/batch_size-32-steps-100005-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/train/model-100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_dir = 'batch_size-32-steps-100005-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101'\n",
    "log_dir = f\"logs/{model_dir}\"\n",
    "\n",
    "# model_dir = 'mi_weight=0.4;embedding_size=512;dataset=xian2018_flowers;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=0;modality_interaction=FILM_T'\n",
    "# log_dir = f\"/mnt/scratch/boris/experiments_zeroshot_pairwise/190427_122014_embedding_size_dataset_text_feature_extractor_number_of_steps_repeat_modality_interaction_mi_weight_xian2018_flowers_test/{model_dir}\"\n",
    "\n",
    "\n",
    "metric_model = MetricLoader(model_path=log_dir, batch_size_image=100, batch_size_text=200)\n",
    "\n",
    "feature_model = ModelLoader(model_path=log_dir, batch_size=None,\n",
    "                            num_images=metric_model.flags.num_images, num_texts=metric_model.flags.num_texts, \n",
    "                            max_text_len=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading split train\n",
      "INFO:root:Loading cached file /mnt/datasets/public/research/cvpr2016_flowers/split_train_xian2018.pkl\n",
      "INFO:root:Loaded cache in 3.845049 sec\n",
      "INFO:root:Loading split val_seen\n",
      "INFO:root:Loading cached file /mnt/datasets/public/research/cvpr2016_flowers/split_val_seen_xian2018.pkl\n",
      "INFO:root:Loaded cache in 1.757812 sec\n",
      "INFO:root:Loading split val_unseen\n",
      "INFO:root:Loading cached file /mnt/datasets/public/research/cvpr2016_flowers/split_val_unseen_xian2018.pkl\n",
      "INFO:root:Loaded cache in 1.757477 sec\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = get_dataset_splits(dataset_name=metric_model.flags.dataset, data_dir=metric_model.flags.data_dir,\n",
    "                                    splits=[metric_model.flags.train_split, \n",
    "                                            metric_model.flags.test_split+\"_seen\", \n",
    "                                            metric_model.flags.test_split+\"_unseen\"], \n",
    "                                    flags=metric_model.flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing train embeddings\n",
      "147it [00:03, 39.66it/s]\n",
      "INFO:root:Computing test embeddings, unseen\n",
      "37it [00:00, 40.03it/s]\n",
      "INFO:root:Computing test embeddings, seen\n",
      "37it [00:00, 40.72it/s]\n",
      "INFO:root:Computing generalized zero-shot performance metrics\n",
      "INFO:root:Searching for 'logs/batch_size-32-steps-100005-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/params.json'\n",
      "INFO:root:Loading existing params.\n",
      " 33%|███▎      | 2/6 [00:00<00:00, 18.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "=================\n",
      "STD seen prototype, seen image:\n",
      "nan\n",
      "STD unseen prototype, seen image:\n",
      "nan\n",
      "STD seen prototype, unseen image:\n",
      "97.6802033168\n",
      "STD unseen prototype, unseen image:\n",
      "93.8505447693\n",
      "=================\n",
      "STD seen prototype, seen image:\n",
      "nan\n",
      "STD unseen prototype, seen image:\n",
      "nan\n",
      "STD seen prototype, unseen image:\n",
      "94.5851890172\n",
      "STD unseen prototype, unseen image:\n",
      "89.3163926371\n",
      "=================\n",
      "STD seen prototype, seen image:\n",
      "nan\n",
      "STD unseen prototype, seen image:\n",
      "nan\n",
      "STD seen prototype, unseen image:\n",
      "94.1561617375\n",
      "STD unseen prototype, unseen image:\n",
      "88.8894088512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 18.78it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype, seen image:\n",
      "nan\n",
      "STD unseen prototype, seen image:\n",
      "nan\n",
      "STD seen prototype, unseen image:\n",
      "93.7038158995\n",
      "STD unseen prototype, unseen image:\n",
      "88.9308668632\n",
      "=================\n",
      "STD seen prototype, seen image:\n",
      "nan\n",
      "STD unseen prototype, seen image:\n",
      "nan\n",
      "STD seen prototype, unseen image:\n",
      "93.668974371\n",
      "STD unseen prototype, unseen image:\n",
      "88.6498308789\n",
      "=================\n",
      "STD seen prototype, seen image:\n",
      "nan\n",
      "STD unseen prototype, seen image:\n",
      "nan\n",
      "STD seen prototype, unseen image:\n",
      "93.6536381085\n",
      "STD unseen prototype, unseen image:\n",
      "88.6574133718\n",
      "=================\n",
      "STD seen prototype, seen image:\n",
      "98.9347098598\n",
      "STD unseen prototype, seen image:\n",
      "95.8301039984\n",
      "STD seen prototype, unseen image:\n",
      "nan\n",
      "STD unseen prototype, unseen image:\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:00<00:00, 20.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "STD seen prototype, seen image:\n",
      "94.7817565133\n",
      "STD unseen prototype, seen image:\n",
      "92.3510630444\n",
      "STD seen prototype, unseen image:\n",
      "nan\n",
      "STD unseen prototype, unseen image:\n",
      "nan\n",
      "=================\n",
      "STD seen prototype, seen image:\n",
      "94.3990874737\n",
      "STD unseen prototype, seen image:\n",
      "91.5304065615\n",
      "STD seen prototype, unseen image:\n",
      "nan\n",
      "STD unseen prototype, unseen image:\n",
      "nan\n",
      "=================\n",
      "STD seen prototype, seen image:\n",
      "94.0668049553\n",
      "STD unseen prototype, seen image:\n",
      "91.5184376361\n",
      "STD seen prototype, unseen image:\n",
      "nan\n",
      "STD unseen prototype, unseen image:\n",
      "nan\n",
      "=================\n",
      "STD seen prototype, seen image:\n",
      "93.9170031821\n",
      "STD unseen prototype, seen image:\n",
      "91.4339155959\n",
      "STD seen prototype, unseen image:\n",
      "nan\n",
      "STD unseen prototype, unseen image:\n",
      "nan\n",
      "=================\n",
      "STD seen prototype, seen image:\n",
      "93.9533600582\n",
      "STD unseen prototype, seen image:\n",
      "91.3455755398\n",
      "STD seen prototype, unseen image:\n",
      "nan\n",
      "STD unseen prototype, unseen image:\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 20.43it/s]\n",
      "INFO:root:Computing classical zero-shot performance metrics, test\n",
      "INFO:root:Searching for 'logs/batch_size-32-steps-100005-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/params.json'\n",
      "INFO:root:Loading existing params.\n",
      "INFO:root:Computing classical zero-shot performance metrics, train\n",
      "INFO:root:Searching for 'logs/batch_size-32-steps-100005-lr-0.1005-opt-sgd-weight_decay-0.001-nfilt-64-image_feature_extractor-resnet101/params.json'\n",
      "INFO:root:Loading existing params.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Loading model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_AP@50/#sentences10': 0.68225806451612903,\n",
       " 'train_Top-1 Acc/#sentences10': 0.67822533972704924,\n",
       " 'train_AP@50/#sentences50': 0.83935483870967753,\n",
       " 'train_Top-1 Acc/#sentences50': 0.88230237241316212,\n",
       " 'train_AP@50/#sentences100': 0.85193548387096774,\n",
       " 'train_Top-1 Acc/#sentences100': 0.90395697481436288,\n",
       " 'train_AP@50/#sentences200': 0.87161290322580653,\n",
       " 'train_Top-1 Acc/#sentences200': 0.92529027982008238,\n",
       " 'train_AP@50/#sentences400': 0.87387096774193562,\n",
       " 'train_Top-1 Acc/#sentences400': 0.92324004314025354,\n",
       " 'train_AP@50/#sentences1000': 0.87516129032258072,\n",
       " 'train_Top-1 Acc/#sentences1000': 0.92492238795771076,\n",
       " 'test_AP@50/#sentences10': 0.28800000000000003,\n",
       " 'test_Top-1 Acc/#sentences10': 0.38084491775671431,\n",
       " 'test_AP@50/#sentences50': 0.32000000000000001,\n",
       " 'test_Top-1 Acc/#sentences50': 0.51591351718579948,\n",
       " 'test_AP@50/#sentences100': 0.35099999999999998,\n",
       " 'test_Top-1 Acc/#sentences100': 0.57218566322697473,\n",
       " 'test_AP@50/#sentences200': 0.36100000000000004,\n",
       " 'test_Top-1 Acc/#sentences200': 0.5896976180440292,\n",
       " 'test_AP@50/#sentences400': 0.37099999999999994,\n",
       " 'test_Top-1 Acc/#sentences400': 0.60316271429670576,\n",
       " 'test_AP@50/#sentences1000': 0.36599999999999994,\n",
       " 'test_Top-1 Acc/#sentences1000': 0.6038809075826046,\n",
       " 'test_U_Top-1 Acc/#sentences10': 0.36175564994056419,\n",
       " 'test_S_Top-1 Acc/#sentences10': 0.55572153287435921,\n",
       " 'test_H_Top-1 Acc/#sentences10': 0.43823521298727264,\n",
       " 'test_U_Top-1 Acc/#sentences50': 0.47461915743243577,\n",
       " 'test_S_Top-1 Acc/#sentences50': 0.72744003331934792,\n",
       " 'test_H_Top-1 Acc/#sentences50': 0.57444255383251741,\n",
       " 'test_U_Top-1 Acc/#sentences100': 0.52268900966273057,\n",
       " 'test_S_Top-1 Acc/#sentences100': 0.72787877947734847,\n",
       " 'test_H_Top-1 Acc/#sentences100': 0.60845040421382035,\n",
       " 'test_U_Top-1 Acc/#sentences200': 0.52908741255356928,\n",
       " 'test_S_Top-1 Acc/#sentences200': 0.74385529506670622,\n",
       " 'test_H_Top-1 Acc/#sentences200': 0.61835378925556084,\n",
       " 'test_U_Top-1 Acc/#sentences400': 0.53766350158076215,\n",
       " 'test_S_Top-1 Acc/#sentences400': 0.75862575644245767,\n",
       " 'test_H_Top-1 Acc/#sentences400': 0.62931228978972209,\n",
       " 'test_U_Top-1 Acc/#sentences1000': 0.54459592627539566,\n",
       " 'test_S_Top-1 Acc/#sentences1000': 0.76119993274478925,\n",
       " 'test_H_Top-1 Acc/#sentences1000': 0.6349329102099861}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_eval, embeddings = feature_model.eval_acc_gzsh(train_loader=dataset_splits[metric_model.flags.train_split], \n",
    "                                                       test_loader_seen=dataset_splits[metric_model.flags.test_split+\"_seen\"],\n",
    "                                                       test_loader_unseen=dataset_splits[metric_model.flags.test_split+\"_unseen\"],\n",
    "                                                       batch_size=32, seen_adjustment=0.03) # 0.015\n",
    "results_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_unseen_text_embeddings, image_embeddings_train, image_embeddings_test = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_unseen_subsets = {}\n",
    "seen_unseen_subsets['seen'] = list(set(dataset_splits[metric_model.flags.train_split].image_classes))\n",
    "seen_unseen_subsets['unseen'] = list(set(dataset_splits[metric_model.flags.test_split].image_classes))\n",
    "\n",
    "seen_unseen_labels = np.concatenate(\n",
    "    [dataset_splits[metric_model.flags.train_split].image_classes, dataset_splits[metric_model.flags.test_split].image_classes], axis=0)\n",
    "\n",
    "class_ids_query = dataset_splits[metric_model.flags.test_split].image_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_num_texts = 1000\n",
    "\n",
    "prototypes = get_prototypes(embeddings=seen_unseen_text_embeddings, labels=seen_unseen_labels, \n",
    "                            vectors_per_protype=current_num_texts)\n",
    "\n",
    "class_ids_prototypes = np.array(list(prototypes.keys()))\n",
    "array_prototypes = np.array(list(prototypes.values()))\n",
    "\n",
    "seen_unseen_flag = np.isin(class_ids_prototypes, seen_unseen_subsets['seen']).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = metric_model.predict_all(image_embeddings=image_embeddings_test, text_embeddings=array_prototypes)\n",
    "dist = dist.transpose()\n",
    "dist = dist * dist\n",
    "dist = dist * (0.0*seen_unseen_flag[:,None] + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Top-1 Acc/#sentences10000': 0.40651929037993684}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {}\n",
    "nn_idxs = np.argmin(dist, axis=0)\n",
    "top1_acc_matches = {}\n",
    "top1_acc_counts = {}\n",
    "top1_acc = {} \n",
    "for query_id, nn_idx in enumerate(nn_idxs):\n",
    "    actual_class = class_ids_query[query_id]\n",
    "    nearest_class = class_ids_prototypes[nn_idx]\n",
    "    top1_acc_matches[actual_class] = top1_acc_matches.get(actual_class, 0.0) + np.float(nearest_class == actual_class)\n",
    "    top1_acc_counts[actual_class] = top1_acc_counts.get(actual_class, 0.0) + 1\n",
    "\n",
    "for key in top1_acc_matches.keys():\n",
    "    top1_acc[key] = top1_acc_matches[key] / top1_acc_counts[key]\n",
    "\n",
    "metrics['Top-1 Acc/#sentences%d'%(10*current_num_texts)] = np.array(list(top1_acc.values())).mean()\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  7.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Top-1 Acc/#sentences10': 0.2819812909754178,\n",
       " 'Top-1 Acc/#sentences30': 0.42396133527416019,\n",
       " 'Top-1 Acc/#sentences100': 0.50692222609057291,\n",
       " 'Top-1 Acc/#sentences200': 0.52879963346939074,\n",
       " 'Top-1 Acc/#sentences300': 0.54425090994636993,\n",
       " 'Top-1 Acc/#sentences400': 0.55583511289720389,\n",
       " 'Top-1 Acc/#sentences500': 0.55823191321101295,\n",
       " 'Top-1 Acc/#sentences1000': 0.56060498938479053}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1_gzsl(support_embeddings=seen_unseen_text_embeddings, \n",
    "          query_embeddings=image_embeddings_test, \n",
    "          class_ids_support=seen_unseen_labels, \n",
    "          class_ids_query=dataset_splits[metric_model.flags.test_split].image_classes, \n",
    "          num_texts=[1, 3, 10, 20, 30, 40, 50, 100], \n",
    "          seen_unseen_subsets=seen_unseen_subsets, distance_metric=metric_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Top-1 Acc/#sentences10': 0.35301990756053186,\n",
       " 'Top-1 Acc/#sentences30': 0.45103795607907587,\n",
       " 'Top-1 Acc/#sentences100': 0.50666833382992293,\n",
       " 'Top-1 Acc/#sentences200': 0.54526526941628528,\n",
       " 'Top-1 Acc/#sentences300': 0.53615094210743031,\n",
       " 'Top-1 Acc/#sentences400': 0.54992179908153682,\n",
       " 'Top-1 Acc/#sentences500': 0.54932806065160544,\n",
       " 'Top-1 Acc/#sentences1000': 0.5510017521452415}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1_gzsl(support_embeddings=seen_unseen_text_embeddings, \n",
    "          query_embeddings=image_embeddings_train, \n",
    "          class_ids_support=seen_unseen_labels, \n",
    "          class_ids_query=dataset_splits[metric_model.flags.train_split].image_classes, \n",
    "          num_texts=[1, 3, 10, 20, 30, 40, 50, 100], \n",
    "          seen_unseen_subsets=seen_unseen_subsets, distance_metric=metric_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_text_embeddings = seen_unseen_text_embeddings[len(dataset_splits[nn_model.flags.train_split].image_classes):]\n",
    "unseen_labels = seen_unseen_labels[len(dataset_splits[nn_model.flags.train_split].image_classes):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Searching for '/mnt/home/boris/experiments_zeroshot_pairwise/190409_000054_embedding_size_film_weight_decay_postmult_text_feature_extractor_number_of_steps_repeat_modality_interaction_num_texts_dropout_encoder_decoder_test/embedding_size=512;film_weight_decay_postmult=0.1;num_texts=10;dropout=0.25;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=1;modality_interaction=FILM/params.json'\n",
      "INFO:root:Loading existing params.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "INFO:tensorflow:Restoring parameters from /mnt/home/boris/experiments_zeroshot_pairwise/190409_000054_embedding_size_film_weight_decay_postmult_text_feature_extractor_number_of_steps_repeat_modality_interaction_num_texts_dropout_encoder_decoder_test/embedding_size=512;film_weight_decay_postmult=0.1;num_texts=10;dropout=0.25;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=1;modality_interaction=FILM/train/model-100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /mnt/home/boris/experiments_zeroshot_pairwise/190409_000054_embedding_size_film_weight_decay_postmult_text_feature_extractor_number_of_steps_repeat_modality_interaction_num_texts_dropout_encoder_decoder_test/embedding_size=512;film_weight_decay_postmult=0.1;num_texts=10;dropout=0.25;text_feature_extractor=cnn_bi_lstm;number_of_steps=100001;repeat=1;modality_interaction=FILM/train/model-100000\n"
     ]
    }
   ],
   "source": [
    "nn_model = NnModelLoader(model_path=log_dir_nn, batch_size_image=10, batch_size_text=len(unseen_text_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = nn_model.predict_all(image_embeddings=image_embeddings_test, text_embeddings=unseen_text_embeddings)\n",
    "dist = dist.transpose()\n",
    "dist = dist * dist\n",
    "# dist = dist * (0.02*seen_unseen_flag[:,None] + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Top-1 Acc/#sentences10000': 0.60447010585803074}"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {}\n",
    "nn_idxs = np.argmin(dist, axis=0)\n",
    "top1_acc_matches = {}\n",
    "top1_acc_counts = {}\n",
    "top1_acc = {} \n",
    "for query_id, nn_idx in enumerate(nn_idxs):\n",
    "    actual_class = class_ids_query[query_id]\n",
    "    nearest_class = unseen_labels[nn_idx]\n",
    "    top1_acc_matches[actual_class] = top1_acc_matches.get(actual_class, 0.0) + np.float(nearest_class == actual_class)\n",
    "    top1_acc_counts[actual_class] = top1_acc_counts.get(actual_class, 0.0) + 1\n",
    "\n",
    "for key in top1_acc_matches.keys():\n",
    "    top1_acc[key] = top1_acc_matches[key] / top1_acc_counts[key]\n",
    "\n",
    "metrics['Top-1 Acc/#sentences%d'%(10*current_num_texts)] = np.array(list(top1_acc.values())).mean()\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11788, 512)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_unseen_text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 150)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "\n",
    "# start = time.time()\n",
    "# text_model = model = api.load(\"word2vec-google-news-300\")\n",
    "# print(\"time loading weights:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "\n",
    "# start = time.time()\n",
    "# text_model1 = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "#     \"/mnt/datasets/public/research/GoogleNews_vectors_negative300/GoogleNews-vectors-negative300.bin.gz\", \n",
    "#     binary=True)\n",
    "# print(\"time loading weights:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_model1['computer', 'IBM', 'bird', 'belly'].shape\n",
    "# text_model1.wv['computer']\n",
    "# text_model1.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/batch_size-24-steps-100000-lr-0.001-opt-adam-weight_decay-0.0005-nfilt-64-image_feature_extractor-simple_res_net\"\n",
    "log_dir = \"logs/batch_size-24-steps-100000-lr-0.001-opt-adam-weight_decay-0.0005-nfilt-64-image_feature_extractor-inception_v3\"\n",
    "\n",
    "exp_dir = \"/mnt/home/boris/experiments_zeroshot_pairwise/\"\n",
    "log_dir = exp_dir + \\\n",
    "\"190206_164521_repeat_optimizer_num_images_number_of_steps_train_batch_size_num_texts_metric_multiplier_init_word2vec_test/repeat=0;optimizer=sgd;num_images=3;number_of_steps=50000;train_batch_size=64;num_texts=3;metric_multiplier_init=5/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/home/boris/experiments_zeroshot_pairwise/190206_164521_repeat_optimizer_num_images_number_of_steps_train_batch_size_num_texts_metric_multiplier_init_word2vec_test/repeat=0;optimizer=sgd;num_images=3;number_of_steps=50000;train_batch_size=64;num_texts=3;metric_multiplier_init=5//train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6e54a5bee77e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# \"190201_145439_repeat_optimizer_num_images_lr_decay_rate_number_of_steps_init_learning_rate_train_batch_size_word_embed_dim_num_texts_sgd_optimizer/repeat=0;optimizer=sgd;num_images=3;word_embed_dim=256;number_of_steps=50000;init_learning_rate=0.1;train_batch_size=24;lr_decay_rate=10;num_texts=5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/home/boris/experiments_zeroshot_pairwise/190206_164521_repeat_optimizer_num_images_number_of_steps_train_batch_size_num_texts_metric_multiplier_init_word2vec_test/repeat=0;optimizer=sgd;num_images=3;number_of_steps=50000;train_batch_size=64;num_texts=3;metric_multiplier_init=5//train'"
     ]
    }
   ],
   "source": [
    "# !ls \"/mnt/home/boris/experiments_zeroshot_pairwise/190201_145439_repeat_optimizer_num_images_lr_decay_rate_number_of_steps_init_learning_rate_train_batch_size_word_embed_dim_num_texts_sgd_optimizer/\" + \\\n",
    "# \"190201_145439_repeat_optimizer_num_images_lr_decay_rate_number_of_steps_init_learning_rate_train_batch_size_word_embed_dim_num_texts_sgd_optimizer/repeat=0;optimizer=sgd;num_images=3;word_embed_dim=256;number_of_steps=50000;init_learning_rate=0.1;train_batch_size=24;lr_decay_rate=10;num_texts=5\"\n",
    "\n",
    "os.listdir(log_dir + '/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = Namespace(load_and_save_params({}, log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Cvpr2016CubLoader(split='trainval')\n",
    "train_loader.load_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.image_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading split test\n",
      "Loading cached file /mnt/datasets/public/research/cvpr2016_cub/split_test.pkl\n",
      "Loaded cache in 14.548067331314087 sec\n"
     ]
    }
   ],
   "source": [
    "test_loader = Cvpr2016CubLoader(split='test')\n",
    "test_loader.load_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2933, 10, 1024)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.image_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelLoader(model_path=log_dir, \n",
    "                    batch_size=None, num_images=10, num_texts=10, max_text_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_embeddings, text_embeddings = model.eval_acc(data_set=test_loader, batch_size=5)\n",
    "\n",
    "ap50, image_embeddings, text_embeddings = model.eval_acc(data_set=test_loader, batch_size=10)\n",
    "print(ap50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.metrics import get_prototypes, ap_at_k_prototypes\n",
    "\n",
    "metrics = ap_at_k_prototypes(support_embeddings=text_embeddings, query_embeddings=image_embeddings, \n",
    "                   class_ids=test_loader.image_classes, k=50, num_texts=[1, 2, 5, 10, 20, 30, 40])\n",
    "\n",
    "\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelLoader(model_path=log_dir, \n",
    "                    batch_size=32, num_images=flags.num_images, num_texts=flags.num_texts, \n",
    "                    max_text_len=train_loader.max_text_len)\n",
    "model.eval_acc_batch(data_set=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_at50, top1_acc = ap_at_k(support_embeddings=text_embeddings, query_embeddings=image_embeddings, \n",
    "                 class_ids=test_loader.image_classes)\n",
    "print(ap_at50, top1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdtree = cKDTree(text_embeddings)\n",
    "ap50 = 0.0\n",
    "print(\"Computing AP@50\")\n",
    "for query_id, query_embedding in enumerate(tqdm(image_embeddings)):\n",
    "    nns, nn_idxs = kdtree.query(query_embedding, k=50)\n",
    "    nn_idxs = nn_idxs.tolist()\n",
    "    \n",
    "    nearest_classes = np.array(test_loader.image_classes)[nn_idxs]\n",
    "    ap50 += sum([c == test_loader.image_classes[query_id] for c in nearest_classes])/50\n",
    "ap50 /= len(image_embeddings)\n",
    "\n",
    "ap50\n",
    "\n",
    "# query_id = 0\n",
    "# nns, nn_idxs = kdtree.query(image_embeddings[query_id], k=50)\n",
    "#     nns_total[query_id] = nns\n",
    "#     nn_idxs_total.append(nn_idxs)\n",
    "\n",
    "# nn_idxs\n",
    "\n",
    "# print(np.array(test_loader.image_classes)[nn_idxs])\n",
    "\n",
    "# print(test_loader.image_classes[query_id])\n",
    "\n",
    "# test_loader.image_classes[query_id]\n",
    "\n",
    "# len(test_loader.image_classes)\n",
    "# len(image_embeddings)\n",
    "# test_loader.image_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_dict = {model.images_pl: images.astype(dtype=np.float32),\n",
    "#                      model.text_pl: texts,\n",
    "#                      model.text_len_pl: text_lengths}\n",
    "\n",
    "# image_embeddings, text_embeddings = model.sess.run([model.image_embeddings, model.text_embeddings], feed_dict)\n",
    "# print(text_embeddings.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for images, texts, text_lengths in train_loader.sequential_evaluation_batches(batch_size=5, num_images=10, num_texts=10):\n",
    "    a = model.predict(images, texts, text_lengths)\n",
    "    a[0] == 0\n",
    "    break\n",
    "    \n",
    "print(a[1]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, texts, text_lengths, _ = train_loader.next_batch(batch_size=2, num_images=10, num_texts=10)\n",
    "model.predict(images, texts, text_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a': 0, 'b':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(a.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
